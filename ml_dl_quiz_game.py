
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML/DL Quiz Game (1200+ Q&A)
---------------------------------
Run locally:
    python ml_dl_quiz_game.py

Features:
- 1200 Q&A across ML, DL, RL, evaluation, data, tools, algorithms, math
- Filter by topic and difficulty
- Randomized questions
- Show/hide answers, record correctness, and track score
- Save your session results to JSON

No external dependencies beyond Python 3 standard library.
"""

import json
import random
import sys
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# -------------------- Embedded Q&A bank (generated) --------------------
_QA_BANK_JSON = [{"question": "State the worst-case complexity of Matrix multiplication (naive).", "answer": "O(n^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of BFS.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use SVM (RBF)?", "answer": "Non-linear decision boundaries using kernel trick.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Prim's MST with heap.", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Define Model pruning.", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Policy in RL.", "answer": "Mapping from states to actions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is the time complexity of Topological Sort?", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Quick Sort (worst).", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is exponential distribution?", "answer": "Time between events in a Poisson process.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is q-value?", "answer": "Expected return from a state–action pair under a policy.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is variance?", "answer": "Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is FTRL?", "answer": "Online learning optimizer mixing Follow-The-Regularized-Leader.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "What is Permutation importance?", "answer": "Drop in performance when a feature is randomly permuted.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of BFS?", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is dropout?", "answer": "Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "State the average-case complexity of PageRank (power iteration).", "answer": "O(kE) for k iterations", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Cohen's Kappa?", "answer": "Classification agreement adjusted for chance.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Model is slow at inference. How to optimize?", "answer": "Quantization, pruning, distillation, operator fusion, and batching.", "topic": "Deployment", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is GroupKFold?", "answer": "Cross-validation splitting by groups to avoid leakage.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of PageRank (power iteration).", "answer": "O(kE) for k iterations", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is a key practical tip for Gradient Boosting?", "answer": "A key tip is: sequential learners that fit residuals.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Explain the role of Replay buffer in RL.", "answer": "Stores experience to break correlation in updates (DQN).", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is Hugging Face Transformers used for?", "answer": "Pretrained models and pipelines for NLP/CV/Audio.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a Self-attention do?", "answer": "Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "What is the time complexity of Union-Find operations (amortized)?", "answer": "Near O(α(n))", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use CatBoost?", "answer": "Handles categorical variables natively; strong default performance.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Heap Sort?", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is bias-variance trade-off?", "answer": "Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Describe Beam search.", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is transfer learning?", "answer": "Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use Triplet loss?", "answer": "Metric learning; enforces anchor-positive closer than anchor-negative by margin.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Explain Gradient clipping.", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is exploration vs exploitation?", "answer": "Trade-off between trying new actions and using known good ones.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is gradient clipping?", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use SpecAugment?", "answer": "Use SpecAugment when time/frequency masking augmentation for speech models.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Explain the role of Soft Actor-Critic in RL.", "answer": "Max-entropy RL optimizing reward + entropy for robust behaviors.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Warm restarts.", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "How do you handle poor calibration?", "answer": "Use temperature scaling, isotonic regression, or focal loss variants.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of BFS.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is UMAP n_neighbors?", "answer": "Balances local vs global structure in UMAP embedding.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "State the average-case complexity of Heap Sort.", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is layer normalization?", "answer": "Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by warm restarts?", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is mini-batch sgd?", "answer": "A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Name one limitation of DBSCAN.", "answer": "It can be limited because requires density parameters eps and min_samples.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What does a Transformer decoder do?", "answer": "Masked self-attention for autoregressive generation.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is random search?", "answer": "Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Quantization.", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Knowledge distillation.", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is q-learning?", "answer": "Off-policy TD control algorithm learning action-value function.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use SHAP values?", "answer": "Use SHAP values when additive feature attributions consistent with game theory.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "State the worst-case complexity of Quick Sort (average).", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Describe Label smoothing.", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is the time complexity of Insertion Sort?", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is subgradient?", "answer": "Generalization of gradient for non-differentiable convex functions.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for t-SNE perplexity.", "answer": "Ensure correct hyperparameters/assumptions when using t-SNE perplexity.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "What is TimeSeriesSplit?", "answer": "Respects temporal order with expanding windows.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for MixUp.", "answer": "Ensure correct hyperparameters/assumptions when using MixUp.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Describe Cyclical learning rates.", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In RL, what is entropy regularization?", "answer": "Encourages exploration by penalizing low-entropy policies.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use DBSCAN?", "answer": "Density-based clustering; finds arbitrary shapes and noise points.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is grid search?", "answer": "Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Partial dependence plot.", "answer": "Ensure correct hyperparameters/assumptions when using Partial dependence plot.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use TimeSeriesSplit?", "answer": "Use TimeSeriesSplit when respects temporal order with expanding windows.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is adagrad?", "answer": "Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why multi-head attention?", "answer": "Enables the model to focus on different representation subspaces in parallel.", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Gradient Boosting?", "answer": "Sequentially corrects errors; strong on tabular data.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is ppo?", "answer": "Proximal Policy Optimization with clipped surrogate objective for stable updates.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use MAE?", "answer": "Regression with outliers; linear penalty on errors.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "When should you use t-SNE?", "answer": "Nonlinear embedding for visualization of high-dimensional data.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In SGD with momentum, what does the `weight_decay` parameter control?", "answer": "It controls the L2 penalty.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Temperature scaling in probability calibration?", "answer": "Single-parameter scaling of logits for simplicity.", "topic": "Evaluation", "difficulty": "advanced", "tags": ["calibration"]}, {"question": "What is k-fold cv?", "answer": "Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is TensorFlow used for?", "answer": "End-to-end ML platform with eager/graph execution and Keras high-level API.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of DFS?", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "When would you use Dice loss?", "answer": "Overlap-based loss popular in segmentation tasks.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is PyTorch used for?", "answer": "Dynamic computation graphs; popular for research and production via TorchScript.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In Adam, what does the `lr` parameter control?", "answer": "It controls the learning rate.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "State the average-case complexity of Insertion Sort.", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of warm restarts?", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Describe Knowledge distillation.", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is bias?", "answer": "Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is mcmc?", "answer": "Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Text classification with small data. Approach?", "answer": "Fine-tune a pretrained transformer with strong regularization and data augmentation.", "topic": "NLP", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "What does a InstanceNorm do?", "answer": "Normalizes per-instance per-channel; common in style transfer.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "What is Weight initialization?", "answer": "Proper schemes (He/Xavier) stabilize gradient flow.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Give a practical tip for TimeSeriesSplit.", "answer": "Ensure correct hyperparameters/assumptions when using TimeSeriesSplit.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use RMSE?", "answer": "Regression where large errors are penalized more.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "What is target encoding and why is it used?", "answer": "Replaces categories with aggregated target statistics; risk of leakage.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Teacher model temperature?", "answer": "Softer targets in distillation; higher temperature smooths probabilities.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a Pooling layer do?", "answer": "Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "What is the time complexity of Matrix multiplication (naive)?", "answer": "O(n^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is quantization?", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is OpenCV used for?", "answer": "Computer vision library for image/video processing.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is momentum?", "answer": "Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "How can decision thresholds be chosen to maximize F1?", "answer": "Use validation curves to pick the threshold that best maximize F1 on held-out data.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["thresholds"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Cyclical learning rates.", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Cyclical learning rates.", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What does a Depthwise separable conv do?", "answer": "Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Entropy regularization in RL.", "answer": "Encourages exploration by penalizing low-entropy policies.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is poisson distribution?", "answer": "Counts of rare events in fixed interval with rate λ.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "What is Batch size?", "answer": "Number of samples per parameter update; affects stability and speed.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "What is stochastic gradient descent?", "answer": "Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use SVM (linear)?", "answer": "High-dimensional, linearly separable data; robust margin maximization.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is bayesian optimization?", "answer": "Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is huber loss?", "answer": "Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is SpecAugment?", "answer": "Time/frequency masking augmentation for speech models.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Explain the role of Advantage function in RL.", "answer": "Q(s,a) − V(s); measures relative value of actions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why add a Residual block in CNNs?", "answer": "To learn residual mapping; eases training of very deep nets.", "topic": "CNN", "difficulty": "intermediate", "tags": ["cnn"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of PPO in RL.", "answer": "Proximal Policy Optimization with clipped surrogate objective for stable updates.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a GRU do?", "answer": "Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "When would you use Contrastive loss?", "answer": "Metric learning for pairs; pulls similar together, pushes dissimilar apart.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is policy?", "answer": "Mapping from states to actions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Linear Regression?", "answer": "Predict continuous outcomes with a linear relationship between features and target.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by label smoothing?", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is l2 regularization?", "answer": "Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use Categorical cross-entropy?", "answer": "For multi-class classification; compares one-hot targets to softmax outputs.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "What is batch normalization?", "answer": "Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is softmax?", "answer": "Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Linear Search.", "answer": "O(n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is strong convexity?", "answer": "Convex with a positive quadratic lower bound; guarantees unique minima and faster rates.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for MSE?", "answer": "mean((y - y_hat)^2)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "Why/When would you use GroupKFold?", "answer": "Use GroupKFold when cross-validation splitting by groups to avoid leakage.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Describe Mixed precision training.", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is dirichlet distribution?", "answer": "Prior over multinomial probabilities; conjugate to categorical.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Give a practical tip for Teacher model temperature.", "answer": "Ensure correct hyperparameters/assumptions when using Teacher model temperature.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle slow convergence?", "answer": "Tune LR/optimizer, use warmup, better initialization, mixed precision.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is regularization?", "answer": "Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use K-Means?", "answer": "Partition data into k clusters by minimizing within-cluster variance.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is data splitting and why is it used?", "answer": "Keep strict separation of train/validation/test.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "What does a Transformer encoder do?", "answer": "Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is gaussian distribution?", "answer": "Continuous bell-shaped distribution defined by mean and variance.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by cyclical learning rates?", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Label smoothing.", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Precision?", "answer": "When false positives are costly (e.g., email spam filters).", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "Explain the role of A3C/A2C in RL.", "answer": "Asynchronous/synchronous actor-critic methods using parallel workers.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a SiLU/Swish do?", "answer": "Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Why/When would you use Permutation importance?", "answer": "Use Permutation importance when drop in performance when a feature is randomly permuted.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is tokenization?", "answer": "Mapping raw text to tokens/ids; e.g., BPE/WordPiece/Unigram.", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is robust scaling and why is it used?", "answer": "Uses robust statistics (median/IQR); resilient to outliers.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by gradient clipping?", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of DFS.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a practical tip for Batch size.", "answer": "Ensure correct hyperparameters/assumptions when using Batch size.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "When would you use MSE?", "answer": "Regression; penalizes squared residuals.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "What does a Fully connected layer do?", "answer": "Dense connections for combining features near output stages.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is kl divergence?", "answer": "Asymmetric measure of difference between two distributions.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "What is CutMix?", "answer": "Augmentation mixing images and labels by cutting and pasting patches.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Permutation importance.", "answer": "Ensure correct hyperparameters/assumptions when using Permutation importance.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use MixUp?", "answer": "Use MixUp when augmentation that convexly combines pairs of samples and labels.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What does a Positional encoding do?", "answer": "Adds order information to sequences in Transformers.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Accuracy?", "answer": "Balanced classes and equal error costs.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "How can decision thresholds be chosen to target specific precision?", "answer": "Use validation curves to pick the threshold that best target specific precision on held-out data.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["thresholds"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle vanishing gradients?", "answer": "Use ReLU/GELU, residual connections, proper initialization, or normalization.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Matthews Correlation Coefficient?", "answer": "Balanced evaluation for binary classification under imbalance.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is a3c/a2c?", "answer": "Asynchronous/synchronous actor-critic methods using parallel workers.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Training loss decreases, val loss increases. What's happening?", "answer": "Overfitting; add regularization, early stopping, or gather more data.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "Give a practical tip for SHAP values.", "answer": "Ensure correct hyperparameters/assumptions when using SHAP values.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is pr-auc?", "answer": "Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What does a GroupNorm do?", "answer": "Normalizes over groups of channels; stable with small batch sizes.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Label noise robustness?", "answer": "Use Label noise robustness when use robust losses, co-teaching, or data cleaning for mislabeled data.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RMSProp, what does the `eps` parameter control?", "answer": "It controls the numerical stability.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Linear Search.", "answer": "O(n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is label smoothing?", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Label noise robustness.", "answer": "Ensure correct hyperparameters/assumptions when using Label noise robustness.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "When should you use UMAP?", "answer": "Fast nonlinear dimensionality reduction preserving local/global structure.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Naive Bayes?", "answer": "Text classification or problems with strong conditional independence assumptions.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "You have extreme class imbalance (1:1000). What do you change first?", "answer": "Use PR-AUC, class weights, resampling, focal loss, and threshold tuning.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of model pruning?", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is shuffling and why is it used?", "answer": "Randomize sample order to avoid sequence bias.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "In RMSProp, what does the `lr` parameter control?", "answer": "It controls the learning rate.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Convex Hull (Graham Scan).", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is standardization and why is it used?", "answer": "Transform features to zero mean and unit variance.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Give an overview of teacher forcing?", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is a key practical tip for Random Forest?", "answer": "A key tip is: reduces variance via bagging and feature randomness.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What is weight decay?", "answer": "L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an example of data leakage.", "answer": "Time-series splitting that allows future info in training.", "topic": "Data", "difficulty": "intermediate", "tags": ["leakage"]}, {"question": "What is one-hot encoding and why is it used?", "answer": "Binary indicator columns for categorical levels.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "What is rmsprop?", "answer": "Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "State the average-case complexity of Quick Sort (average).", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is mixed precision training?", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Binary Search.", "answer": "O(log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Why/When would you use Weight initialization?", "answer": "Use Weight initialization when proper schemes (he/xavier) stabilize gradient flow.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is data augmentation?", "answer": "Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "When should you use ROC-AUC?", "answer": "Overall ranking quality; may be optimistic under heavy imbalance.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is CatBoost used for?", "answer": "Boosting that handles categorical features natively with strong defaults.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "What is the formula for Precision?", "answer": "TP / (TP + FP)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "What is t-SNE perplexity?", "answer": "Controls effective number of neighbors in t-SNE.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Define Label smoothing.", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Gradient clipping.", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Floyd-Warshall.", "answer": "O(V^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is MixUp?", "answer": "Augmentation that convexly combines pairs of samples and labels.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "What is SHAP values?", "answer": "Additive feature attributions consistent with game theory.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Teacher forcing.", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is dimensionality reduction and why is it used?", "answer": "PCA/UMAP/autoencoders to reduce feature space.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Gradients are NaN. What to check?", "answer": "Learning rate too high, numerical stability (epsilon), exploding activations; try clipping and lower LR.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Fisher information.", "answer": "Ensure correct hyperparameters/assumptions when using Fisher information.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a DropPath/Stochastic Depth do?", "answer": "Randomly drop residual blocks to regularize very deep networks.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is early stopping?", "answer": "Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is recall?", "answer": "TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Merge Sort?", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is nesterov momentum?", "answer": "Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Name one limitation of GMM.", "answer": "It can be limited because uses EM to estimate component means/covariances.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "State the worst-case complexity of KMP pattern matching.", "answer": "O(n + m)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Explain Model pruning.", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Cosine similarity.", "answer": "Ensure correct hyperparameters/assumptions when using Cosine similarity.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In Adam, what does the `beta2` parameter control?", "answer": "It controls the 2nd-moment decay.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "Why/When would you use Elastic Net?", "answer": "Use Elastic Net when combines l1 and l2 penalties to balance sparsity and stability.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How can decision thresholds be chosen to maximize expected utility under asymmetric costs?", "answer": "Use validation curves to pick the threshold that best maximize expected utility under asymmetric costs on held-out data.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["thresholds"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for RMSE?", "answer": "sqrt(MSE)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Matrix multiplication (naive).", "answer": "O(n^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "When should you use XGBoost?", "answer": "Efficient gradient boosted trees with regularization; excellent for tabular competitions.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Quick Sort (worst).", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is cross-entropy?", "answer": "Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is target network?", "answer": "Stabilizes Q-learning by using delayed parameters for bootstrapping.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "State the average-case complexity of Dijkstra with binary heap.", "answer": "O((V + E) log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Why/When would you use Cosine similarity?", "answer": "Use Cosine similarity when measures angle-based similarity between vectors.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is mae?", "answer": "Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is warm restarts?", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why add a Dropout in CNNs?", "answer": "To reduce overfitting by randomly zeroing activations.", "topic": "CNN", "difficulty": "intermediate", "tags": ["cnn"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by model pruning?", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a practical tip for Gradient accumulation.", "answer": "Ensure correct hyperparameters/assumptions when using Gradient accumulation.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Partial dependence plot?", "answer": "Use Partial dependence plot when average effect of a feature on prediction over dataset.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is adam?", "answer": "Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "State the average-case complexity of Merge Sort.", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use IoU/Jaccard loss?", "answer": "Penalizes non-overlap in segmentation/detection.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "What is the time complexity of Floyd-Warshall?", "answer": "O(V^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "State the worst-case complexity of Topological Sort.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Bellman-Ford.", "answer": "O(V * E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian Process", "answer": "Gaussian Process: Distribution over functions with mean and kernel capturing similarity.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is value function?", "answer": "Expected return from a state under a policy.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Why/When would you use ICE plot?", "answer": "Use ICE plot when individual conditional expectation curves for per-sample sensitivity.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Your validation accuracy is high but test accuracy drops. Why?", "answer": "Possible leakage via hyperparameter overfitting or distribution shift; revisit validation protocol.", "topic": "Generalization", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "In RL, what is markov decision process?", "answer": "Framework with states, actions, transition dynamics, rewards, discount factor.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use KNN?", "answer": "Non-parametric classification/regression; sensitive to feature scaling and k choice.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Explain the role of Q-learning in RL.", "answer": "Off-policy TD control algorithm learning action-value function.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is cyclical learning rates?", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is a key practical tip for K-Means?", "answer": "A key tip is: sensitive to initialization; use k-means++.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "How can decision thresholds be chosen to target specific recall?", "answer": "Use validation curves to pick the threshold that best target specific recall on held-out data.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["thresholds"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is policy gradient?", "answer": "Optimizes policy parameters via gradient of expected return.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Explain the role of Discount factor (gamma) in RL.", "answer": "Weights future rewards; small gamma favors short-term rewards.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Union-Find operations (amortized).", "answer": "Near O(α(n))", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Describe Model pruning.", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle covariate shift?", "answer": "Use domain adaptation, reweighting, or collect more representative data.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What is the formula for Specificity?", "answer": "TN / (TN + FP)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "What does a LayerNorm do?", "answer": "Normalizes across features per token/sample; stable training in Transformers.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is missing value imputation and why is it used?", "answer": "Fill missing values (mean/median/most frequent or model-based).", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is representation learning?", "answer": "Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is singular value decomposition?", "answer": "Factorization A=UΣVᵀ; reveals rank and principal directions.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is ONNX used for?", "answer": "Open format for representing ML models for cross-framework deployment.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Topological Sort.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is stratified cv?", "answer": "Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is gaussian process?", "answer": "Distribution over functions with mean and kernel capturing similarity.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "What is Ray used for?", "answer": "Distributed execution for Python/ML workloads.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "What is attention masking?", "answer": "Mask prevents attending to future tokens (causal) or pads (padding mask).", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "Give an overview of gradient clipping?", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Cosine similarity?", "answer": "Measures angle-based similarity between vectors.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Teacher forcing.", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "When should you use LightGBM?", "answer": "Histogram-based boosting; very fast on large tabular datasets.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In RL, what is soft actor-critic?", "answer": "Max-entropy RL optimizing reward + entropy for robust behaviors.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of mixed precision training?", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is beam search?", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What do we mean by teacher forcing?", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is supervised learning?", "answer": "Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Dijkstra with binary heap.", "answer": "O((V + E) log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Define Mixed precision training.", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Overfitting?", "answer": "Overfitting: Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of KMP pattern matching.", "answer": "O(n + m)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Knowledge distillation.", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Floyd-Warshall.", "answer": "O(V^3)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Describe Gradient clipping.", "answer": "Clamp gradient norms/values to prevent explosion.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a GELU do?", "answer": "Smooth, non-linear activation used in modern Transformers.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for SpecAugment.", "answer": "Ensure correct hyperparameters/assumptions when using SpecAugment.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "What is spaCy used for?", "answer": "Industrial-strength NLP library for tokenization, tagging, NER.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Name one limitation of Random Forest.", "answer": "It can be limited because reduces variance via bagging and feature randomness.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Binary Search.", "answer": "O(log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Value function in RL.", "answer": "Expected return from a state under a policy.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Batch size?", "answer": "Use Batch size when number of samples per parameter update; affects stability and speed.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Explain the role of Markov Decision Process in RL.", "answer": "Framework with states, actions, transition dynamics, rewards, discount factor.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is text vectorization and why is it used?", "answer": "Bag-of-Words/TF-IDF/embeddings to convert text to numeric features.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is mean squared error?", "answer": "Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a practical tip for FTRL.", "answer": "Ensure correct hyperparameters/assumptions when using FTRL.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Give a concise definition of Attention pooling.", "answer": "Attention pooling: Aggregates variable-length features by learned attention weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Selection Sort.", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Gradient accumulation?", "answer": "Use Gradient accumulation when sum grads across steps to simulate larger batches.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Convex Hull (Graham Scan)?", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Elastic Net.", "answer": "Ensure correct hyperparameters/assumptions when using Elastic Net.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "State the average-case complexity of Selection Sort.", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is MLflow used for?", "answer": "Experiment tracking, model registry, and reproducible runs.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is loss function?", "answer": "Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is label smoothing?", "answer": "Prevents overconfidence by distributing some probability mass to non-target classes.", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "Define Beam search.", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is Gradient accumulation?", "answer": "Sum grads across steps to simulate larger batches.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use Binary cross-entropy?", "answer": "For binary classification; measures divergence between true and predicted Bernoulli.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Why add a Bottleneck block in CNNs?", "answer": "To reduce channels then expand; efficient deep residual designs.", "topic": "CNN", "difficulty": "intermediate", "tags": ["cnn"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is ICE plot?", "answer": "Individual Conditional Expectation curves for per-sample sensitivity.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "When should you use Log loss?", "answer": "Penalizes incorrect confident predictions in classification.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an example of data leakage.", "answer": "Duplicated rows existing across train and test.", "topic": "Data", "difficulty": "intermediate", "tags": ["leakage"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Kruskal's MST.", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is Fisher information?", "answer": "Expected curvature of log-likelihood; informs parameter uncertainty.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "In simple terms, what is Mean squared error?", "answer": "Mean squared error: Average of squared residuals; common for regression.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is image normalization and why is it used?", "answer": "Scale pixels, channel-wise mean/std; improves training stability.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "What is activation function?", "answer": "Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "State the average-case complexity of Prim's MST with heap.", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is outlier handling and why is it used?", "answer": "Clip, transform, or model robustly to reduce outlier impact.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "In RL, what is discount factor (gamma)?", "answer": "Weights future rewards; small gamma favors short-term rewards.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Poisson distribution", "answer": "Poisson distribution: Counts of rare events in fixed interval with rate λ.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use MAPE?", "answer": "Forecasting; beware near-zero targets.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Exponential distribution?", "answer": "Exponential distribution: Time between events in a Poisson process.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for CutMix.", "answer": "Ensure correct hyperparameters/assumptions when using CutMix.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Label noise robustness?", "answer": "Use robust losses, co-teaching, or data cleaning for mislabeled data.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Give an overview of label smoothing?", "answer": "Distribute small probability mass to non-target classes to regularize.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is f1-score?", "answer": "Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is em algorithm?", "answer": "Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is l1 regularization?", "answer": "Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is variational inference?", "answer": "Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Union-Find operations (amortized).", "answer": "Near O(α(n))", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Policy gradient in RL.", "answer": "Optimizes policy parameters via gradient of expected return.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is advantage function?", "answer": "Q(s,a) − V(s); measures relative value of actions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is Isotonic regression in probability calibration?", "answer": "Non-parametric calibration fitting a monotonic function.", "topic": "Evaluation", "difficulty": "advanced", "tags": ["calibration"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle overfitting?", "answer": "Use regularization, data augmentation, early stopping, simpler model.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Fisher information?", "answer": "Use Fisher information when expected curvature of log-likelihood; informs parameter uncertainty.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is reinforcement learning?", "answer": "Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is confusion matrix?", "answer": "Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Hierarchical Clustering?", "answer": "Builds a tree (dendrogram) of clusters; agglomerative/divisive.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is the time complexity of Linear Search?", "answer": "O(n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use MAE?", "answer": "Regression with outliers; linear penalty.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Give a practical tip for UMAP n_neighbors.", "answer": "Ensure correct hyperparameters/assumptions when using UMAP n_neighbors.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In Adam, what does the `beta1` parameter control?", "answer": "It controls the 1st-moment decay.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Mixed precision training.", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Explain Warm restarts.", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In RL, what is temporal difference learning?", "answer": "Bootstraps from current estimates to learn value functions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Brier Score?", "answer": "Calibration of probabilistic predictions in binary outcomes.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an example of data leakage.", "answer": "Using test data statistics to scale training features.", "topic": "Data", "difficulty": "intermediate", "tags": ["leakage"]}, {"question": "How do you handle label noise?", "answer": "Clean labels, robust losses, co-teaching, or confidence-based filtering.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "When should you use R^2?", "answer": "Proportion of variance explained in regression.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Define Quantization.", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is cross-validation?", "answer": "Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by mixed precision training?", "answer": "Use float16/32 to accelerate training with minimal loss in quality.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is DropPath/Stochastic Depth?", "answer": "DropPath/Stochastic Depth: Randomly drop residual blocks to regularize very deep networks.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Bellman equation in RL.", "answer": "Recursive relationship for value functions under optimality or policy evaluation.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Bias-variance trade-off", "answer": "Bias-variance trade-off: Balancing underfitting (high bias) and overfitting (high variance) to minimize total error.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is teacher forcing?", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a practical tip for GroupKFold.", "answer": "Ensure correct hyperparameters/assumptions when using GroupKFold.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "What is condition number?", "answer": "Ratio of largest to smallest singular value; measures numerical stability.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a practical tip for Weight initialization.", "answer": "Ensure correct hyperparameters/assumptions when using Weight initialization.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "In simple terms, what is L2 regularization?", "answer": "L2 regularization: Penalizes squared weights; encourages small, distributed weights.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of PageRank (power iteration)?", "answer": "O(kE) for k iterations", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "When should you use PR-AUC?", "answer": "Preferred under heavy class imbalance focusing on positives.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is hold-out validation?", "answer": "Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is the time complexity of Strassen multiplication?", "answer": "O(n^log2(7)) ≈ O(n^2.807)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What do we mean by quantization?", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle underfitting?", "answer": "Increase capacity, better features, longer training, reduce regularization.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "State the worst-case complexity of Heap Sort.", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is convex function?", "answer": "Function where line segment between any two points lies above the graph.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Platt scaling in probability calibration?", "answer": "Fit logistic regression on validation logits to calibrate probabilities.", "topic": "Evaluation", "difficulty": "advanced", "tags": ["calibration"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Random Forest?", "answer": "Ensemble of trees to reduce variance; strong baselines.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is precision?", "answer": "TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is jensen's inequality?", "answer": "For convex f, f(E[X]) ≤ E[f(X)].", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a Multi-head attention do?", "answer": "Multiple attention subspaces for richer relations.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "State the worst-case complexity of Radix Sort.", "answer": "O(nk)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Tabular dataset with many categories. What baseline?", "answer": "CatBoost/LightGBM with careful encoding; consider target encoding cautiously.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "In simple terms, what is Transformer decoder?", "answer": "Transformer decoder: Masked self-attention for autoregressive generation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of KMP pattern matching?", "answer": "O(n + m)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of A* search?", "answer": "Depends on heuristic; often exponential in worst case", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is XGBoost used for?", "answer": "Optimized gradient boosting for decision trees; strong on tabular data.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Decision Tree?", "answer": "Interpretable rules; handles nonlinearity and interactions.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Transformer encoder", "answer": "Transformer encoder: Self-attention + feedforward + normalization; parallelizable sequence modeling.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Selection Sort?", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give an overview of cyclical learning rates?", "answer": "Vary LR within bounds to improve exploration and convergence.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: SiLU/Swish", "answer": "SiLU/Swish: Activation x * sigmoid(x); smooth and effective in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Counting Sort?", "answer": "O(n + k)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In RMSProp, what does the `rho` parameter control?", "answer": "It controls the decay of squared grads.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "What do we mean by knowledge distillation?", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is bellman equation?", "answer": "Recursive relationship for value functions under optimality or policy evaluation.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Loss function", "answer": "Loss function: Quantifies discrepancy between predictions and targets (e.g., MSE, cross-entropy).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for Recall?", "answer": "TP / (TP + FN)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "Give a concise definition of MAE.", "answer": "MAE: Mean absolute error; robust to outliers compared to MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Positional encoding?", "answer": "Positional encoding: Adds order information to sequences in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an example of data leakage.", "answer": "Target encoding without out-of-fold strategy.", "topic": "Data", "difficulty": "intermediate", "tags": ["leakage"]}, {"question": "State the worst-case complexity of Merge Sort.", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is semi-supervised learning?", "answer": "Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "What is Partial dependence plot?", "answer": "Average effect of a feature on prediction over dataset.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "In RL, what is replay buffer?", "answer": "Stores experience to break correlation in updates (DQN).", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is the time complexity of Radix Sort?", "answer": "O(nk)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Cross-validation.", "answer": "Cross-validation: Resampling strategy to estimate generalization by training/validating on multiple folds.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Counting Sort.", "answer": "O(n + k)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In SGD with momentum, what does the `lr` parameter control?", "answer": "It controls the learning rate.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "Briefly define: Momentum", "answer": "Momentum: Accelerates SGD by accumulating a velocity vector in parameter space.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is dqn?", "answer": "Deep Q-Network combining Q-learning with CNNs and replay/target networks.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Image segmentation metrics to report?", "answer": "Dice, IoU, precision, recall, and per-class metrics.", "topic": "CV", "difficulty": "intermediate", "tags": ["scenario"]}, {"question": "Give a concise definition of Dirichlet distribution.", "answer": "Dirichlet distribution: Prior over multinomial probabilities; conjugate to categorical.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Isolation Forest?", "answer": "Anomaly detection via random partitioning; efficient on high-dim data.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is gradient descent?", "answer": "Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Fully connected layer", "answer": "Fully connected layer: Dense connections for combining features near output stages.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Actor-Critic in RL.", "answer": "Combines policy (actor) and value function (critic) for variance reduction.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "When should you use Logistic Regression?", "answer": "Binary classification with probabilistic outputs via sigmoid.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for R^2?", "answer": "1 - SS_res / SS_tot", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is feature selection and why is it used?", "answer": "Filter/wrapper/embedded methods to remove irrelevant features.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is map estimation?", "answer": "Maximize posterior using prior and likelihood.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Explain the role of Target network in RL.", "answer": "Stabilizes Q-learning by using delayed parameters for bootstrapping.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Multi-head attention", "answer": "Multi-head attention: Multiple attention subspaces for richer relations.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Optuna used for?", "answer": "Hyperparameter optimization framework with efficient search.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Name one limitation of K-Means.", "answer": "It can be limited because sensitive to initialization; use k-means++.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What does a Convolutional layer do?", "answer": "Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use t-SNE perplexity?", "answer": "Use t-SNE perplexity when controls effective number of neighbors in t-sne.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Radix Sort.", "answer": "O(nk)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is the time complexity of Bellman-Ford?", "answer": "O(V * E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "When should you use F1-score?", "answer": "Imbalanced classes needing a balance of precision and recall.", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Quick Sort (worst)?", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Adam.", "answer": "Adam: Adaptive optimization combining momentum and RMSProp ideas; uses first/second moment estimates.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Kruskal's MST.", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use Teacher model temperature?", "answer": "Use Teacher model temperature when softer targets in distillation; higher temperature smooths probabilities.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "State the worst-case complexity of Strassen multiplication.", "answer": "O(n^log2(7)) ≈ O(n^2.807)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use Recall?", "answer": "When false negatives are costly (e.g., disease screening).", "topic": "Evaluation", "difficulty": "intermediate", "tags": ["metrics"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Keras used for?", "answer": "High-level API for building neural networks quickly on top of TF.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "State the average-case complexity of Bellman-Ford.", "answer": "O(V * E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of LayerNorm.", "answer": "LayerNorm: Normalizes across features per token/sample; stable training in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is eigenvalue?", "answer": "Scalar λ such that Av = λv for eigenvector v.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "In simple terms, what is Bias?", "answer": "Bias: Error from erroneous assumptions or overly simple models.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: MCMC", "answer": "MCMC: Sampling-based methods (e.g., Metropolis-Hastings, HMC) to approximate posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Feature engineering", "answer": "Feature engineering: Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In SGD with momentum, what does the `momentum` parameter control?", "answer": "It controls the past gradient contribution.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "Give a concise definition of Learning rate.", "answer": "Learning rate: Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is lipschitz continuity?", "answer": "A function whose rate of change is bounded by a constant; aids stability and generalization.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Kruskal's MST?", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "How do you handle class imbalance?", "answer": "Use class weights, focal loss, resampling, or threshold tuning.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What is the formula for Accuracy?", "answer": "(TP + TN) / (TP + TN + FP + FN)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "What does a Dilated convolution do?", "answer": "Expands receptive field without losing resolution.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "What is bernoulli distribution?", "answer": "Binary outcomes with parameter p for success probability.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "What is underfitting?", "answer": "Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for MAE?", "answer": "mean(|y - y_hat|)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "What is roc-auc?", "answer": "Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "How do you handle exploding gradients?", "answer": "Use gradient clipping, lower learning rate, better initialization, or normalization.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Explain the role of DQN in RL.", "answer": "Deep Q-Network combining Q-learning with CNNs and replay/target networks.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is kkt conditions?", "answer": "Optimality conditions for constrained optimization problems.", "topic": "Math", "difficulty": "advanced", "tags": ["math"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Data augmentation.", "answer": "Data augmentation: Synthetic data transformations (flip/crop/noise) to improve generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Counting Sort.", "answer": "O(n + k)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "State the worst-case complexity of A* search.", "answer": "Depends on heuristic; often exponential in worst case", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use FTRL?", "answer": "Use FTRL when online learning optimizer mixing follow-the-regularized-leader.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is InstanceNorm?", "answer": "InstanceNorm: Normalizes per-instance per-channel; common in style transfer.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Name one limitation of Gradient Boosting.", "answer": "It can be limited because sequential learners that fit residuals.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What is a key practical tip for GMM?", "answer": "A key tip is: uses EM to estimate component means/covariances.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "What is bayes' theorem?", "answer": "P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is min-max scaling and why is it used?", "answer": "Rescales features to a fixed range, usually [0,1].", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a Residual connection do?", "answer": "Adds input to output to ease gradient flow in deep nets.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is Stochastic gradient descent?", "answer": "Stochastic gradient descent: Uses a single (or mini-batch) sample to approximate gradient for each update.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Residual connection.", "answer": "Residual connection: Adds input to output to ease gradient flow in deep nets.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Mini-batch SGD.", "answer": "Mini-batch SGD: A compromise between batch and stochastic methods; updates using small batches.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why use positional encodings in Transformers?", "answer": "Self-attention is permutation-invariant; positional encodings inject order.", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "What is imbalanced data?", "answer": "Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is unsupervised learning?", "answer": "Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Describe Teacher forcing.", "answer": "Feed ground-truth tokens to speed sequence model training.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Batch normalization", "answer": "Batch normalization: Normalizes layer activations per mini-batch to stabilize and speed up training.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In Adam, what does the `eps` parameter control?", "answer": "It controls the numerical stability.", "topic": "Optimization", "difficulty": "beginner", "tags": ["optimizer"]}, {"question": "In simple terms, what is EM algorithm?", "answer": "EM algorithm: Iteratively performs E-step and M-step to maximize likelihood with latent variables.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use CutMix?", "answer": "Use CutMix when augmentation mixing images and labels by cutting and pasting patches.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is feature engineering?", "answer": "Creating informative features from raw data to boost model performance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Weight decay.", "answer": "Weight decay: L2 regularization applied during optimizer step.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of knowledge distillation?", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Give a concise definition of GELU.", "answer": "GELU: Smooth, non-linear activation used in modern Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of MAP estimation.", "answer": "MAP estimation: Maximize posterior using prior and likelihood.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Softmax.", "answer": "Softmax: Converts logits into a probability distribution over classes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Dilated convolution?", "answer": "Dilated convolution: Expands receptive field without losing resolution.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is warmup?", "answer": "Gradually increase LR at start to stabilize training of deep nets.", "topic": "Transformers", "difficulty": "advanced", "tags": ["transformer"]}, {"question": "What is learning rate?", "answer": "Controls the step size during optimization; too large diverges, too small is slow.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "In simple terms, what is Precision?", "answer": "Precision: TP / (TP + FP): proportion of predicted positives that are correct.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What does a Attention pooling do?", "answer": "Aggregates variable-length features by learned attention weights.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of quantization?", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What do we mean by beam search?", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Why add a BatchNorm in CNNs?", "answer": "To stabilize activations and gradients across the batch.", "topic": "CNN", "difficulty": "intermediate", "tags": ["cnn"]}, {"question": "When should you use One-Class SVM?", "answer": "Learns a boundary around normal data for anomaly detection.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is stratification and why is it used?", "answer": "Maintain class proportions in splits.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Pooling layer.", "answer": "Pooling layer: Reduces spatial resolution (e.g., max/avg pooling) to gain invariance and efficiency.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is knowledge distillation?", "answer": "Train a smaller student to mimic a larger teacher model.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gradient descent", "answer": "Gradient descent: Iteratively updates parameters in the direction of negative gradient of the loss.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Regularization?", "answer": "Regularization: Penalizing model complexity to reduce overfitting (e.g., L1/L2/dropout).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Convolutional layer?", "answer": "Convolutional layer: Applies learned kernels over spatial/temporal dimensions to extract local features.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Recall.", "answer": "Recall: TP / (TP + FN): proportion of actual positives that are retrieved.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Grid search", "answer": "Grid search: Exhaustive search over hyperparameter grid.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain Beam search.", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Confusion matrix", "answer": "Confusion matrix: Table of TP/FP/TN/FN summarizing classification results.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Supervised learning", "answer": "Supervised learning: Learning from labeled data to map inputs to known outputs.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of PR-AUC.", "answer": "PR-AUC: Area under precision-recall curve; useful with class imbalance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Convex Hull (Graham Scan).", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of A* search.", "answer": "Depends on heuristic; often exponential in worst case", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "How do you handle data leakage?", "answer": "Strictly separate train/val/test; avoid target info in features/processing.", "topic": "Troubleshooting", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Give a concise definition of Self-attention.", "answer": "Self-attention: Computes pairwise interactions among tokens/features to aggregate information.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Transfer learning?", "answer": "Transfer learning: Reusing knowledge from a source task/model to a related target task.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is ROC-AUC?", "answer": "ROC-AUC: Probability a classifier ranks a random positive higher than a random negative.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for Balanced accuracy?", "answer": "(TPR + TNR) / 2", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "What does a LSTM do?", "answer": "RNN variant with gates to capture long-term dependencies.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["architecture"]}, {"question": "In simple terms, what is Adagrad?", "answer": "Adagrad: Adapts learning rates based on past gradients; can lead to diminishing step sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Temporal Difference learning in RL.", "answer": "Bootstraps from current estimates to learn value functions.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: GroupNorm", "answer": "GroupNorm: Normalizes over groups of channels; stable with small batch sizes.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Bayes' theorem", "answer": "Bayes' theorem: P(A|B) = P(B|A) P(A) / P(B); updates beliefs with evidence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Elastic Net?", "answer": "Combines L1 and L2 penalties to balance sparsity and stability.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["explain"]}, {"question": "Give a concise definition of Bayesian optimization.", "answer": "Bayesian optimization: Model-based search (e.g., Gaussian Process) to choose promising hyperparameters.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Early stopping?", "answer": "Early stopping: Stop training when validation performance stops improving.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Why/When would you use UMAP n_neighbors?", "answer": "Use UMAP n_neighbors when balances local vs global structure in umap embedding.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is binomial distribution?", "answer": "Number of successes in n Bernoulli trials.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Briefly define: Depthwise separable conv", "answer": "Depthwise separable conv: Factorizes conv to reduce parameters and computation (e.g., MobileNet).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of DFS.", "answer": "O(V + E)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "In simple terms, what is Unsupervised learning?", "answer": "Unsupervised learning: Learning patterns from unlabeled data, such as clustering or density estimation.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Dropout.", "answer": "Dropout: Randomly deactivates neurons during training to prevent co-adaptation and overfitting.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give an overview of beam search?", "answer": "Heuristic decoding that keeps top-k partial sequences at each step.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: RMSProp", "answer": "RMSProp: Adaptive learning rate per-parameter using moving average of squared gradients.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Nesterov momentum?", "answer": "Nesterov momentum: Computes gradient at the lookahead position; often yields faster convergence.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use Sparse categorical cross-entropy?", "answer": "Like categorical CE but with integer class labels.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "Briefly define: Imbalanced data", "answer": "Imbalanced data: Classes have skewed frequencies; use resampling, class weights, appropriate metrics.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is actor-critic?", "answer": "Combines policy (actor) and value function (critic) for variance reduction.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Layer normalization?", "answer": "Layer normalization: Normalizes across features per sample; common in Transformers.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the formula for F1-score?", "answer": "2 * (precision * recall) / (precision + recall)", "topic": "Evaluation", "difficulty": "beginner", "tags": ["formula"]}, {"question": "When should you use Gaussian Mixture Models?", "answer": "Soft clustering with probabilistic component assignments.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "What is maximum likelihood estimation?", "answer": "Choose parameters maximizing likelihood of observed data.", "topic": "Probabilistic ML", "difficulty": "advanced", "tags": ["bayesian"]}, {"question": "Give a practical tip for ICE plot.", "answer": "Ensure correct hyperparameters/assumptions when using ICE plot.", "topic": "ML Practice", "difficulty": "intermediate", "tags": ["tip"]}, {"question": "Give a concise definition of Underfitting.", "answer": "Underfitting: Model is too simple to capture the underlying pattern in data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: F1-score", "answer": "F1-score: Harmonic mean of precision and recall.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of GRU.", "answer": "GRU: Simplified gated RNN capturing long-term dependencies with fewer parameters than LSTM.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In RL, what is sarsa?", "answer": "On-policy TD control algorithm updating with actions actually taken.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is a key practical tip for DBSCAN?", "answer": "A key tip is: requires density parameters eps and min_samples.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["practical"]}, {"question": "Briefly define: L1 regularization", "answer": "L1 regularization: Encourages sparsity in parameters by penalizing absolute values.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When should you use PCA?", "answer": "Dimensionality reduction preserving variance along principal components.", "topic": "Classical ML", "difficulty": "intermediate", "tags": ["use-case"]}, {"question": "In simple terms, what is LSTM?", "answer": "LSTM: RNN variant with gates to capture long-term dependencies.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Variational inference.", "answer": "Variational inference: Optimize a tractable surrogate to approximate complex posteriors.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Hold-out validation", "answer": "Hold-out validation: Split data into train/validation/test sets to estimate generalization.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is Weights & Biases used for?", "answer": "Experiment tracking/visualization/collaboration platform.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "In simple terms, what is Maximum likelihood estimation?", "answer": "Maximum likelihood estimation: Choose parameters maximizing likelihood of observed data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is scikit-learn used for?", "answer": "Python ML library for classical models, preprocessing, pipelines, and metrics.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Stratified CV.", "answer": "Stratified CV: Maintains label proportions in each fold; useful for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the average-case complexity of Strassen multiplication.", "answer": "O(n^log2(7)) ≈ O(n^2.807)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Describe Warm restarts.", "answer": "Periodically reset scheduler to escape sharp minima (CosineAnnealingWarmRestarts).", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Briefly define: Huber loss", "answer": "Huber loss: Combines MSE/MAE behavior; less sensitive to outliers than MSE.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Bernoulli distribution?", "answer": "Bernoulli distribution: Binary outcomes with parameter p for success probability.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Describe Quantization.", "answer": "Reduce numerical precision (e.g., int8) to speed up inference.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "What is overfitting?", "answer": "Model memorizes noise, performing well on train but poorly on unseen data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition"]}, {"question": "Give a concise definition of Variance.", "answer": "Variance: Error from sensitivity to small fluctuations in training data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is Activation function?", "answer": "Activation function: Introduces nonlinearity (ReLU, sigmoid, tanh, GELU, SiLU).", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of SARSA in RL.", "answer": "On-policy TD control algorithm updating with actions actually taken.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "In simple terms, what is Random search?", "answer": "Random search: Samples random combinations; often more efficient than grid with large spaces.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "In simple terms, what is K-fold CV?", "answer": "K-fold CV: Split data into k folds; train on k-1 and validate on the remaining fold iteratively.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "State the worst-case complexity of Insertion Sort.", "answer": "O(n^2)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Reinforcement learning", "answer": "Reinforcement learning: Learning by interacting with an environment to maximize cumulative reward.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Quick Sort (average)?", "answer": "O(n log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Give a concise definition of Binomial distribution.", "answer": "Binomial distribution: Number of successes in n Bernoulli trials.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Semi-supervised learning.", "answer": "Semi-supervised learning: Learning from a small amount of labeled data combined with a large amount of unlabeled data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Explain the role of Q-value in RL.", "answer": "Expected return from a state–action pair under a policy.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "What is LightGBM used for?", "answer": "Fast gradient boosting with histogram-based splits; good for large data.", "topic": "Tools", "difficulty": "beginner", "tags": ["tools"]}, {"question": "What is the time complexity of Dijkstra with binary heap?", "answer": "O((V + E) log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "What is the time complexity of Prim's MST with heap?", "answer": "O(E log V)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is ordinal encoding and why is it used?", "answer": "Integer encodes order of categories; beware unintended ordinality.", "topic": "Data", "difficulty": "beginner", "tags": ["preprocessing"]}, {"question": "Briefly define: Gaussian distribution", "answer": "Gaussian distribution: Continuous bell-shaped distribution defined by mean and variance.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "When would you use Focal loss?", "answer": "Down-weights easy examples to focus on hard/imbalanced cases.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["loss"]}, {"question": "What is model pruning?", "answer": "Remove unimportant weights/filters to compress models.", "topic": "Deep Learning", "difficulty": "intermediate", "tags": ["training"]}, {"question": "Explain the role of Exploration vs exploitation in RL.", "answer": "Trade-off between trying new actions and using known good ones.", "topic": "Reinforcement Learning", "difficulty": "advanced", "tags": ["RL"]}, {"question": "Briefly define: Cross-entropy", "answer": "Cross-entropy: Measures difference between two distributions; common for classification.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "Give a concise definition of Representation learning.", "answer": "Representation learning: Learning features/embeddings that capture essential structure of the data.", "topic": "Foundations", "difficulty": "beginner", "tags": ["definition", "variant"]}, {"question": "What is the time complexity of Binary Search?", "answer": "O(log n)", "topic": "Algorithms", "difficulty": "beginner", "tags": ["complexity"]}, {"question": "Why add a Inception module in CNNs?", "answer": "To parallel mixed receptive fields to capture multi-scale features.", "topic": "CNN", "difficulty": "intermediate", "tags": ["cnn"]}]

# -------------------- Data structures ----------------------------------
@dataclass
class QAItem:
    question: str
    answer: str
    topic: str
    difficulty: str
    tags: List[str]

def load_bank() -> List[QAItem]:
    bank = []
    for item in _QA_BANK_JSON:
        bank.append(QAItem(
            question=item["question"],
            answer=item["answer"],
            topic=item.get("topic", "General"),
            difficulty=item.get("difficulty", "intermediate"),
            tags=item.get("tags", []),
        ))
    return bank

def filter_bank(bank: List[QAItem], topic: Optional[str], difficulty: Optional[str]) -> List[QAItem]:
    res = bank
    if topic:
        topic_lower = topic.strip().lower()
        res = [q for q in res if q.topic.strip().lower() == topic_lower]
    if difficulty:
        diff_lower = difficulty.strip().lower()
        res = [q for q in res if q.difficulty.strip().lower() == diff_lower]
    return res

def pick_questions(bank: List[QAItem], n: int, seed: int = 1337) -> List[QAItem]:
    rnd = random.Random(seed)
    if n >= len(bank):
        rnd.shuffle(bank)
        return bank
    return rnd.sample(bank, n)

def ask_loop(items: List[QAItem]) -> Dict[str, Any]:
    print(f"Loaded {len(items)} questions. Let's play!\n")
    correct = 0
    total = 0
    history = []

    for idx, qa in enumerate(items, 1):
        print(f"[{idx}/{len(items)}] Topic={qa.topic}  Difficulty={qa.difficulty}")
        print("Q:", qa.question)
        input("Press Enter to reveal the answer...")
        print("A:", qa.answer)
        mark = input("Did you get it right? (y/n, Enter defaults to 'n'): ").strip().lower()
        got = (mark == 'y')
        correct += 1 if got else 0
        total += 1
        history.append({
            "question": qa.question,
            "answer": qa.answer,
            "topic": qa.topic,
            "difficulty": qa.difficulty,
            "correct": got
        })
        print("—" * 72)

    print(f"Score: {correct} / {total}  ({(100.0*correct/total):.2f}%)\n")
    return {
        "score": correct,
        "total": total,
        "percent": (100.0*correct/total) if total else 0.0,
        "history": history
    }

def save_results(results: Dict[str, Any], path: str) -> None:
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"Saved results to {path}\n")

def main(argv: List[str]) -> int:
    import argparse
    parser = argparse.ArgumentParser(description="ML/DL Quiz Game")
    parser.add_argument("-t", "--topic", type=str, default=None,
                        help="Filter by topic (e.g., 'Deep Learning', 'Algorithms', 'Evaluation', 'Tools', 'Reinforcement Learning', 'Foundations', 'Data', 'CNN', 'Transformers', 'Math', 'Classical ML', 'ML Practice', 'Generalization', 'Troubleshooting', 'Deployment', 'NLP', 'CV')")
    parser.add_argument("-d", "--difficulty", type=str, default=None,
                        help="Filter by difficulty (beginner/intermediate/advanced)")
    parser.add_argument("-n", "--num", type=int, default=20, help="Number of questions to ask")        
    parser.add_argument("-s", "--seed", type=int, default=1337, help="Random seed for selection order")        
    parser.add_argument("-o", "--output", type=str, default=None, help="Optional JSON to save your results")        

    args = parser.parse_args(argv)

    bank = load_bank()
    if args.topic or args.difficulty:
        bank = filter_bank(bank, topic=args.topic, difficulty=args.difficulty)
        if not bank:
            print("No questions match your filters. Try different --topic/--difficulty.")
            return 1

    items = pick_questions(bank, n=args.num, seed=args.seed)
    results = ask_loop(items)

    if args.output:
        save_results(results, args.output)

    print("Thanks for playing!\n")
    return 0

if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
